#!/usr/bin/env python3
"""
Q-Learning Assignment - Grid World Challenge
‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î Q-learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô

‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ assignment ‡∏ï‡πà‡∏≤‡∏á‡πÜ
"""

from simple_q_learning import SimpleGridWorld, SimpleQLearning

def assignment_1_basic():
    """
    Assignment 1: Basic Q-Learning
    ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô Q-learning ‡πÉ‡∏ô Grid World 4x4 ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    print("=== Assignment 1: Basic Q-Learning ===")
    print()
    
    # TODO: ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÅ‡∏•‡∏∞ agent
    env = SimpleGridWorld(size=4)
    agent = SimpleQLearning(
        n_states=16,
        n_actions=4,
        learning_rate=0.1, #Test 0.01 ‡∏Å‡∏±‡∏ö 0.5
        discount=0.9,
        epsilon=0.1
    )   
    
    print("Grid World Setup:")
    env.print_grid()
    
    # TODO: ‡∏ù‡∏∂‡∏Å agent
    print("Training...")
    agent.train(env, episodes=500)
    
    # TODO: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•
    print("\nTesting trained agent:")
    reward, steps, path = agent.test(env, show_path=False)
    print(f"Total reward: {reward}")
    print(f"Steps taken: {steps}")
    
    # ‡πÅ‡∏™‡∏î‡∏á Q-table ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
    print("\nQ-Table (first 8 states):")
    print("State |   ‚Üë   |   ‚Üì   |   ‚Üê   |   ‚Üí   ")
    print("-" * 40)
    for state in range(8):
        q_vals = agent.q_table[state]
        print(f"{state:5d} | {q_vals[0]:5.2f} | {q_vals[1]:5.2f} | {q_vals[2]:5.2f} | {q_vals[3]:5.2f}")
    
    print("\n--- Questions for Assignment 1 ---")
    print("1. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏≥‡πÑ‡∏° Q-value ‡∏Ç‡∏≠‡∏á state ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ goal ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤")
    print("     ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡∏™‡∏°‡∏Å‡∏≤‡∏£ Bellman, ‡∏Ñ‡πà‡∏≤‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (+10) ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á state ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤")
    print("      states ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏Å‡πâ‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á goal ‡∏à‡∏∂‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤")
    print("2. ‡∏ó‡∏≥‡πÑ‡∏° epsilon-greedy policy ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ")
    print("     epsilon-greedy = ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏° action ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á (exploration) ")
    print("     ‡∏ó‡∏≥‡πÉ‡∏´‡πâ agent ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏•‡∏≠‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà ‡πÜ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ ‚Äú‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö")
    print("3. ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô learning rate ‡πÄ‡∏õ‡πá‡∏ô 0.01 ‡πÅ‡∏•‡∏∞ 0.5 ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•")
    print("     learning rate 0.01 Agent ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q-value ‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢ ‚Üí ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å Q-table ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≠‡∏¢ ‡πÜ ‡πÇ‡∏ï‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤")
    print("     Learning rate 0.5  Agent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å ‚Üí reward ‡∏û‡∏∏‡πà‡∏á‡∏ñ‡∏∂‡∏á ~9.5 ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà Q-value ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏•‡∏á‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£")
def assignment_2_parameter_study():
    """
    Assignment 2: Parameter Study
    ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    """
    print("=== Assignment 2: Parameter Study ===")
    print()
    
    # ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö learning rates ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    learning_rates = [0.01, 0.1, 0.3, 0.7]
    #epsilons = [0.01, 0.1, 0.3, 0.7
    #gammas = [0.5, 0.7, 0.9, 0.99]
    print("Testing different learning rates:")
    
    for lr in learning_rates:
        env = SimpleGridWorld(size=4)
        agent = SimpleQLearning(
            n_states=16,
            n_actions=4,
            learning_rate=lr,
            discount=0.9,
            epsilon=0.1
        )
        
        agent.train(env, episodes=500)
        reward, steps, _ = agent.test(env, show_path=False)
        
        print(f"Learning Rate {lr}: Final reward = {reward:.2f}, Steps = {steps}")
    
    print("\n--- Questions for Assignment 2 ---")
    print("1. Learning rate ‡πÑ‡∏´‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î? ‡∏ó‡∏≥‡πÑ‡∏°?")
    print("     - 0.01 ‚Üí ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")
    print("     - 0.7 ‚Üí ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏ï‡πà Q-values ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£")
    print("     - 0.1 ‡∏´‡∏£‡∏∑‡∏≠ 0.3 ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚Üí ‡∏™‡∏°‡∏î‡∏∏‡∏• ‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£")
    print("2. ‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö epsilon values: 0.01, 0.1, 0.3, 0.7")
    print("     - 0.01: ‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤")
    print("     - 0.1: ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏î‡∏µ (exploit 90%, explore 10%) ‚úÖ")
    print("     - 0.3: explore ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏¢‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô")
    print("     - 0.7: explore ‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ agent ‡πÄ‡∏î‡∏¥‡∏ô‡∏°‡∏±‡πà‡∏ß ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢ exploit")
    print("3. ‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö discount factor: 0.5, 0.7, 0.9, 0.99")
    print("     - 0.5: ‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏Å‡∏•‡πâ ‡πÜ ‡πÑ‡∏°‡πà‡∏°‡∏∏‡πà‡∏á‡πÑ‡∏õ goal")
    print("     - 0.7: ‡∏™‡∏ô‡πÉ‡∏à‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ö‡πâ‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏≤‡∏Å")
    print("     - 0.9: ‡∏™‡∏°‡∏î‡∏∏‡∏• ‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï ‚úÖ")
    print("     - 0.99: ‡∏™‡∏ô‡πÉ‡∏à‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡∏•‡∏á")

def assignment_3_environment_design():
    """
    Assignment 3: Environment Design
    ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö environment ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    """
    print("=== Assignment 3: Environment Design ===")
    print()
    
    # TODO: ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÉ‡∏´‡∏°‡πà
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Grid World ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
    # class CustomGridWorld(SimpleGridWorld):
    #     def __init__(self):
    #         super().__init__(size=6)   # ‡∏ó‡∏≥ grid 6x6
    #     # ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡πÄ‡∏≠‡∏á
    #         self.obstacles = [(1,1), (1,2), (2,3), (3,3), (4,2), (4,4)]
    #     # ‡∏õ‡∏£‡∏±‡∏ö reward/penalty
    #         self.goal_reward = 15
    #         self.obstacle_penalty = -8

    class CustomGridWorld(SimpleGridWorld):
        def __init__(self):
            super().__init__(size=5)
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡πÉ‡∏´‡∏°‡πà
            self.obstacles = [(1, 1), (1, 2), (2, 1), (3, 3)]
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô reward structure
            self.goal_reward = 20
            self.obstacle_penalty = -10
    
    env = CustomGridWorld()
    print("Custom Grid World:")
    env.print_grid()
    
    agent = SimpleQLearning(
        n_states=25,
        # n_states=36,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.2
    )
    
    agent.train(env, episodes=1000)
    reward, steps, _ = agent.test(env, show_path=False)
    print(f"Custom environment result: Reward = {reward:.2f}, Steps = {steps}")
    
    print("\n--- Tasks for Assignment 3 ---")
    print("1. ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Grid World ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á (‡∏Ç‡∏ô‡∏≤‡∏î, ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ, rewards)")
    print("2. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏±‡∏ö standard environment")
    print("     - 4√ó4: agent ‡πÉ‡∏ä‡πâ 6 steps ‡∏ñ‡∏∂‡∏á goal, reward ‚âà 9.5")
    print("     - 6√ó6: agent ‡πÉ‡∏ä‡πâ 10 steps ‡∏ñ‡∏∂‡∏á goal, reward ‚âà 9.1")
    print("     - ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠ ‡∏Å‡∏£‡∏¥‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí agent ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡πâ‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô, reward ‡πÄ‡∏•‡∏¢‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢")
    print("3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ environment design ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ learning ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£")
    print("     -‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö environment ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡∏∞‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏¢‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ episodes ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô, exploration ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏∂‡πâ‡∏ô")

def assignment_4_advanced():
    """
    Assignment 4: Advanced Modifications
    ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á algorithm ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà
    """
    print("=== Assignment 4: Advanced Modifications ===")
    print()
    
    # TODO: ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à
    
    print("Choose one of the following topics:")
    print("1. Implement SARSA algorithm ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Q-Learning")
    print("2. Add epsilon decay strategy ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤")
    print("3. Implement Double Q-Learning")
    print("4. Add experience replay")
    print("5. Create multi-goal environment")
    print("6. Implement priority sweeping")
    
    print("\nExample: Simple SARSA Implementation")
    
    class SARSAAgent(SimpleQLearning):
        """SARSA Agent - On-policy learning"""
        
        def train_episode(self, env, max_steps=100):
            state = env.reset()
            action = self.get_action(state)  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏£‡∏Å
            total_reward = 0
            
            for _ in range(max_steps):
                next_state, reward, done = env.step(action)
                next_action = self.get_action(next_state) if not done else 0
                
                # SARSA update: ‡πÉ‡∏ä‡πâ actual next action ‡πÅ‡∏ó‡∏ô max
                target = reward + self.gamma * self.q_table[next_state][next_action]
                error = target - self.q_table[state][action]
                self.q_table[state][action] += self.lr * error
                
                total_reward += reward
                state, action = next_state, next_action
                
                if done:
                    break
            
            return total_reward, 0
    
    print("SARSA vs Q-Learning comparison example implemented above.")

    print("\n=== ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö SARSA vs Q-Learning ===")

    env = SimpleGridWorld(size=5, difficulty='normal')

    # Q-Learning agent
    q_agent = SimpleQLearning(
        n_states=env.size * env.size,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.2
    )
    q_agent.train(env, episodes=800)
    q_reward, q_steps, _ = q_agent.test(env, show_path=False)
    print(f"Q-Learning -> Reward={q_reward:.2f}, Steps={q_steps}")

    # SARSA agent
    sarsa_agent = SARSAAgent(
        n_states=env.size * env.size,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.2
    )
    sarsa_agent.train(env, episodes=800)
    s_reward, s_steps, _ = sarsa_agent.test(env, show_path=False)
    print(f"SARSA      -> Reward={s_reward:.2f}, Steps={s_steps}")

    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    if s_reward > q_reward:
        print("\nSARSA ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô environment ‡∏ô‡∏µ‡πâ ‚úÖ")
    elif q_reward > s_reward:
        print("\nQ-Learning ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô environment ‡∏ô‡∏µ‡πâ ‚úÖ")
    else:
        print("\n‡∏ú‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏™‡∏π‡∏™‡∏µ‡∏Å‡∏±‡∏ô")


def bonus_visualization():
    """
    Bonus: Enhanced Visualization
    ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à)
    """
    print("=== Bonus: Enhanced Visualization ===")
    print()
    
    # TODO: ‡πÉ‡∏ä‡πâ matplotlib ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü learning curve
    # TODO: ‡∏™‡∏£‡πâ‡∏≤‡∏á animation ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    # TODO: ‡πÅ‡∏™‡∏î‡∏á heatmap ‡∏Ç‡∏≠‡∏á Q-values

    import matplotlib.pyplot as plt
    import numpy as np


    """
    Bonus: Enhanced Visualization
    """
    print("=== Bonus: Enhanced Visualization ===\n")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÅ‡∏•‡∏∞ agent
    from simple_q_learning import SimpleGridWorld, SimpleQLearning
    env = SimpleGridWorld(size=4)
    agent = SimpleQLearning(
        n_states=env.size * env.size,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.3
    )

    # ‡∏ù‡∏∂‡∏Å agent ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö reward
    episodes = 500
    agent.train(env, episodes=episodes)

    # üìà Plot Learning Curve
    plt.figure(figsize=(8,5))
    plt.plot(agent.episode_rewards, label="Episode reward")
    plt.title("Q-Learning Performance (Learning Curve)")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.grid(True)
    plt.legend()
    plt.show()

    # üî• Heatmap ‡∏Ç‡∏≠‡∏á Q-values (‡∏Ñ‡πà‡∏≤ max Q ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ state)
    q_values = [max(agent.q_table[s]) for s in range(agent.n_states)]
    q_grid = np.array(q_values).reshape(env.size, env.size)

    plt.figure(figsize=(6,6))
    plt.imshow(q_grid, cmap="YlGnBu", origin="upper")
    plt.colorbar(label="Max Q-value")
    plt.title("Heatmap of Max Q-values per State")
    for r in range(env.size):
        for c in range(env.size):
            plt.text(c, r, f"{q_grid[r,c]:.1f}",
                     ha="center", va="center", color="black")
    plt.show()

    print("\n‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Learning Curve ‡πÅ‡∏•‡∏∞ Heatmap ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")

        
    print("Ideas for enhanced visualization:")
    print("1. Plot learning curves with matplotlib")
    print("2. Create heatmap of Q-values")
    print("3. Animate the learning process")
    print("4. Show value function as 3D surface")
    print("5. Create policy visualization with arrows")
    
    print("\nSample code for learning curve:")
    print("""
import matplotlib.pyplot as plt

def plot_learning_curve(episode_rewards):
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards)
    plt.title('Q-Learning Performance')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.grid(True)
    plt.show()
    """)

def main():
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å assignment ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥"""
    print("üéì Q-Learning Assignments")
    print("=========================")
    print()
    
    assignments = {
        '1': assignment_1_basic,
        '2': assignment_2_parameter_study,
        '3': assignment_3_environment_design,
        '4': assignment_4_advanced,
        '5': bonus_visualization
    }
    
    while True:
        print("Available assignments:")
        print("1. Basic Q-Learning (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)")
        print("2. Parameter Study (‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå)")
        print("3. Environment Design (‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö environment)")
        print("4. Advanced Modifications (‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á)")
        print("5. Bonus: Visualization (‡πÄ‡∏™‡∏£‡∏¥‡∏°)")
        print("6. Exit")
        print()
        
        choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å assignment (1-6): ").strip()
        
        if choice in assignments:
            print()
            assignments[choice]()
            print("\n" + "="*50 + "\n")
        elif choice == '6':
            print("Good luck with your assignments! üöÄ")
            break
        else:
            print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1-6")

if __name__ == "__main__":
    main()
